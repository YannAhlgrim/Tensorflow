{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.10 from \"c:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\python.exe\"\n  * The NumPy version is: \"1.23.5\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: No module named 'numpy.core._multiarray_umath'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\numpy\\core\\__init__.py:23\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m multiarray\n\u001b[0;32m     24\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\numpy\\core\\multiarray.py:10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfunctools\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m overrides\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _multiarray_umath\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\numpy\\core\\overrides.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_multiarray_umath\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     add_docstring, implement_array_function, _get_implementing_args)\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inspect\u001b[39;00m \u001b[39mimport\u001b[39;00m getargspec\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfds\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m TextVectorization\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\tensorflow_datasets\\__init__.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m _TIMESTAMP_IMPORT_STARTS \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabsl\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[1;32m---> 43\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_tfds_logging\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m call_metadata \u001b[39mas\u001b[39;00m _call_metadata\n\u001b[0;32m     46\u001b[0m _metadata \u001b[39m=\u001b[39m _call_metadata\u001b[39m.\u001b[39mCallMetadata()\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\tensorflow_datasets\\core\\__init__.py:22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Allow to use `tfds.core.Path` in dataset implementation which seems more\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# natural than having to import a third party module.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39metils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mepath\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m community\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_builder\u001b[39;00m \u001b[39mimport\u001b[39;00m BeamBasedBuilder\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_builder\u001b[39;00m \u001b[39mimport\u001b[39;00m BuilderConfig\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\tensorflow_datasets\\core\\community\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# coding=utf-8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Copyright 2023 The TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m\"\"\"Community dataset API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommunity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhuggingface_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m mock_builtin_to_use_gfile\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommunity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhuggingface_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m mock_huggingface_import\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommunity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m \u001b[39mimport\u001b[39;00m builder_cls_from_module\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\tensorflow_datasets\\core\\community\\huggingface_wrapper.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munittest\u001b[39;00m \u001b[39mimport\u001b[39;00m mock\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39metils\u001b[39;00m \u001b[39mimport\u001b[39;00m epath\n\u001b[1;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_builder\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_info\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m download\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39metils\u001b[39;00m \u001b[39mimport\u001b[39;00m epath\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m constants\n\u001b[1;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_info\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_metadata\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m decode\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_info.py:47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39metils\u001b[39;00m \u001b[39mimport\u001b[39;00m epath\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m constants\n\u001b[1;32m---> 47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m file_adapters\n\u001b[0;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m lazy_imports_lib\n\u001b[0;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m naming\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\tensorflow_datasets\\core\\file_adapters.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, ClassVar, Dict, Iterable, List, Optional, Type, Union\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39metils\u001b[39;00m \u001b[39mimport\u001b[39;00m epath\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m type_utils\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_imports_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m array_record_module\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_imports_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m tensorflow \u001b[39mas\u001b[39;00m tf\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\__init__.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39munits\u001b[39;00m \u001b[39mimport\u001b[39;00m Size\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m docs\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m tree_utils \u001b[39mas\u001b[39;00m tree\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgcs_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m gcs_path\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m apply_colormap\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\tree_utils.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable, TypeVar\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm_utils \u001b[39mas\u001b[39;00m tqdm\n\u001b[1;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m type_utils\n\u001b[0;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtree\u001b[39;00m\n\u001b[0;32m     26\u001b[0m Tree \u001b[39m=\u001b[39m type_utils\u001b[39m.\u001b[39mTree\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\type_utils.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n\u001b[1;32m---> 21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_imports_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m tensorflow \u001b[39mas\u001b[39;00m tf\n\u001b[0;32m     24\u001b[0m _symbols_to_exclude \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mglobals\u001b[39m()\u001b[39m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\numpy\\__init__.py:140\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39m# Allow distributors to run custom init code\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init\n\u001b[1;32m--> 140\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[0;32m    141\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n",
      "File \u001b[1;32mc:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\lib\\site-packages\\numpy\\core\\__init__.py:49\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[39mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39m\"\"\"\u001b[39m \u001b[39m%\u001b[39m (sys\u001b[39m.\u001b[39mversion_info[\u001b[39m0\u001b[39m], sys\u001b[39m.\u001b[39mversion_info[\u001b[39m1\u001b[39m], sys\u001b[39m.\u001b[39mexecutable,\n\u001b[0;32m     48\u001b[0m         __version__, exc)\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[0;32m     50\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     \u001b[39mfor\u001b[39;00m envkey \u001b[39min\u001b[39;00m env_added:\n",
      "\u001b[1;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.10 from \"c:\\Users\\yann.MSI\\anaconda3\\envs\\data_science\\python.exe\"\n  * The NumPy version is: \"1.23.5\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: No module named 'numpy.core._multiarray_umath'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, LSTM, GRU, SimpleRNN, Dense, Activation, Embedding, Bidirectional\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = tfds.load(\"imdb_reviews\", as_supervised=True, split=[\"train\", \"test\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec = TextVectorization(\n",
    "        max_tokens=20_000,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=80\n",
    "    )\n",
    "\n",
    "def preprocess_model() -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,), dtype=tf.string))\n",
    "    model.add(text_vec)\n",
    "    return model\n",
    "\n",
    "def one_hot(y:tf.Tensor) -> tf.Tensor:\n",
    "    y = tf.one_hot(tf.cast(y, tf.int32), depth=2)\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    return y\n",
    "\n",
    "preprocessing_model = preprocess_model()\n",
    "words = train_dataset.map(lambda x,y: x)\n",
    "text_vec.adapt(words)\n",
    "train_dataset = train_dataset.map(lambda x,y: (tf.expand_dims(tf.cast(preprocessing_model(x, training=False), tf.int32), axis=-1), tf.expand_dims(y, axis=-1)))\n",
    "test_dataset = test_dataset.map(lambda x,y: (preprocessing_model(x, training=False), one_hot(y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.int64, name=None))>\n",
      "tf.Tensor(\n",
      "[[   11]\n",
      " [   14]\n",
      " [   34]\n",
      " [  412]\n",
      " [  384]\n",
      " [   18]\n",
      " [   90]\n",
      " [   28]\n",
      " [10690]\n",
      " [    8]\n",
      " [   33]\n",
      " [ 1322]\n",
      " [ 3560]\n",
      " [   42]\n",
      " [  487]\n",
      " [11832]\n",
      " [  191]\n",
      " [   24]\n",
      " [   85]\n",
      " [  152]\n",
      " [   19]\n",
      " [   11]\n",
      " [  217]\n",
      " [  316]\n",
      " [   28]\n",
      " [   65]\n",
      " [  240]\n",
      " [  214]\n",
      " [    8]\n",
      " [  489]\n",
      " [   54]\n",
      " [   65]\n",
      " [   85]\n",
      " [  112]\n",
      " [   96]\n",
      " [   22]\n",
      " [ 5596]\n",
      " [   11]\n",
      " [   93]\n",
      " [  642]\n",
      " [  743]\n",
      " [   11]\n",
      " [   18]\n",
      " [    7]\n",
      " [   34]\n",
      " [  394]\n",
      " [ 9522]\n",
      " [  170]\n",
      " [ 2464]\n",
      " [  408]\n",
      " [    2]\n",
      " [   88]\n",
      " [ 1216]\n",
      " [  137]\n",
      " [   66]\n",
      " [  144]\n",
      " [   51]\n",
      " [    2]\n",
      " [    1]\n",
      " [ 7558]\n",
      " [   66]\n",
      " [  245]\n",
      " [   65]\n",
      " [ 2870]\n",
      " [   16]\n",
      " [18297]\n",
      " [ 2860]\n",
      " [    1]\n",
      " [    1]\n",
      " [ 1426]\n",
      " [ 5050]\n",
      " [    3]\n",
      " [   40]\n",
      " [    1]\n",
      " [ 1579]\n",
      " [   17]\n",
      " [ 3560]\n",
      " [   14]\n",
      " [  158]\n",
      " [   19]], shape=(80, 1), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[  10]\n",
      " [  26]\n",
      " [  75]\n",
      " [ 617]\n",
      " [   6]\n",
      " [ 776]\n",
      " [2355]\n",
      " [ 299]\n",
      " [  95]\n",
      " [  19]\n",
      " [  11]\n",
      " [   7]\n",
      " [ 604]\n",
      " [ 662]\n",
      " [   6]\n",
      " [   4]\n",
      " [2129]\n",
      " [   5]\n",
      " [ 180]\n",
      " [ 571]\n",
      " [  63]\n",
      " [1403]\n",
      " [ 107]\n",
      " [2410]\n",
      " [   3]\n",
      " [3905]\n",
      " [  21]\n",
      " [   2]\n",
      " [   1]\n",
      " [   3]\n",
      " [ 252]\n",
      " [  41]\n",
      " [4781]\n",
      " [   4]\n",
      " [ 169]\n",
      " [ 186]\n",
      " [  21]\n",
      " [  11]\n",
      " [4259]\n",
      " [  10]\n",
      " [1507]\n",
      " [2355]\n",
      " [  80]\n",
      " [   2]\n",
      " [  20]\n",
      " [  14]\n",
      " [1973]\n",
      " [   2]\n",
      " [ 114]\n",
      " [ 943]\n",
      " [  14]\n",
      " [1740]\n",
      " [1300]\n",
      " [ 594]\n",
      " [   3]\n",
      " [ 356]\n",
      " [ 180]\n",
      " [ 446]\n",
      " [   6]\n",
      " [ 596]\n",
      " [  19]\n",
      " [  17]\n",
      " [  57]\n",
      " [1775]\n",
      " [   5]\n",
      " [  49]\n",
      " [  14]\n",
      " [4002]\n",
      " [  98]\n",
      " [  42]\n",
      " [ 134]\n",
      " [  10]\n",
      " [ 934]\n",
      " [  10]\n",
      " [ 194]\n",
      " [  26]\n",
      " [1026]\n",
      " [ 171]\n",
      " [   5]\n",
      " [   2]], shape=(80, 1), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "for text, label in train_dataset.take(2):\n",
    "    print(text)\n",
    "    print(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(\n",
    "    input_shape: tuple[int, int],\n",
    "    num_classes: int,\n",
    "    embedding_dim: int\n",
    ")->Model:\n",
    "    \n",
    "    input_text = Input(shape=input_shape)\n",
    "    x = Embedding(\n",
    "        input_dim= 20_000, #vocab size\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=80 #sequence length\n",
    "    )(input_text)\n",
    "    x = LSTM(units=80, return_sequences=False)(x)\n",
    "    x = Dense(units=80)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dense(units=1)(x)\n",
    "    output = Activation(\"sigmoid\")(x)\n",
    "    model = Model(inputs=[input_text], outputs=[output])\n",
    "    \n",
    "    opt = Adam(learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=opt,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_shape = (80) #sequence length\n",
    "num_classes = 2\n",
    "epochs = 10\n",
    "batch_size = 512\n",
    "embedding_dim = 50\n",
    "model = create_model(input_shape=input_shape, num_classes=num_classes, embedding_dim=embedding_dim)\n",
    "model.fit(x=train_dataset, verbose=1, batch_size=batch_size, epochs=epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
